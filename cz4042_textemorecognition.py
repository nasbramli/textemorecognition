# -*- coding: utf-8 -*-
"""CZ4042_textemorecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t_GeYTMrqTdSHCDjLcVUQOORcOcjDpei
"""

!pip install text_hammer
!pip install gensim==4.0

import pandas as pd
import numpy as np
import spacy
spacy.load("en_core_web_sm")
import tqdm as notebook_tqdm
import text_hammer as th
import tensorflow as tf
import ipywidgets
import os
import gensim

!git clone https://github.com/vinayakumarr/WASSA-2017.git

# Combining the training and validation dataset as no.of samples is less in this dataset

train_path = './WASSA-2017/wassa/data/training/'
valid_path = './WASSA-2017/wassa/data/validation/'
test_path = './WASSA-2017/wassa/data/testing/'

def gen_df(path):
    for idx, i in enumerate(os.listdir(path)):
        if idx == 0:
            lst = []
        temp_df = pd.read_csv(path + i, sep='\t')
        lst += temp_df,
    return pd.concat(lst)

train_df = gen_df(train_path)
valid_df = gen_df(valid_path)
test_df = gen_df(test_path)

train_df = pd.concat([train_df, valid_df])

train_df

train_df.info()

train_df.reset_index(drop=True)
test_df.reset_index(drop=True)

train_df = train_df[['tweet', 'tweettype']]
test_df = test_df[['tweet', 'tweettype']]

train_df.tweettype.value_counts()

# Import label encoder
from sklearn import preprocessing

# label_encoder object knows how to understand word labels.
label_encoder = preprocessing.LabelEncoder()

# Encode labels in column 'species'.
train_df['tweettype']= label_encoder.fit_transform(train_df['tweettype'])
test_df['tweettype']= label_encoder.transform(test_df['tweettype']) ;

train_df.tweettype.value_counts()

# fear - 1, anger - 0, joy - 2, sadness- 3

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from tqdm._tqdm_notebook import tqdm_notebook
# tqdm_notebook.pandas()
# def text_preprocessing(df,col_name):
#     column = col_name
#     df[column] = df[column].progress_apply(lambda x:str(x).lower())
#     df[column] = df[column].progress_apply(lambda x: th.cont_exp(x))
#     df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))
#     df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))
#     df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))
#     df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))
#     df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,
#     return(df)
# 
# train_df_new = text_preprocessing(train_df, 'tweet')
# test_df_new = text_preprocessing(test_df, 'tweet');

from keras.utils import to_categorical

X_train = train_df['tweet']
y_train = to_categorical(train_df['tweettype'])
X_test = test_df['tweet']
y_test = to_categorical(test_df['tweettype'])

X_train[0]

y_train[0]

from keras.preprocessing.text import Tokenizer
num_words = 40000 # this means 10000 unique words can be taken
tokenizer=Tokenizer(num_words,lower=True)
tokenizer.fit_on_texts(X_train)

from keras_preprocessing.sequence import pad_sequences

X_train=tokenizer.texts_to_sequences(X_train) # this converts texts into some numeric sequences
X_train=pad_sequences(X_train,maxlen=300,padding='post')
X_test=tokenizer.texts_to_sequences(X_test) # this converts texts into some numeric sequences
X_test=pad_sequences(X_test,maxlen=300,padding='post')

import gensim.downloader as api
glove_gensim  = api.load('glove-wiki-gigaword-100') #100 dimension

vector_size = 100
gensim_weight_matrix = np.zeros((num_words ,vector_size))
gensim_weight_matrix.shape

for word, index in tokenizer.word_index.items():
    if index < num_words: # since index starts with zero
        if word in glove_gensim.key_to_index:
            gensim_weight_matrix[index] = glove_gensim[word]
        else:
            gensim_weight_matrix[index] = np.zeros(100)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional, Conv1D
import tensorflow
from tensorflow.keras.layers import Dropout

EMBEDDING_DIM = 100
class_num = 4
model = Sequential()
model.add(Embedding(input_dim = num_words,
 output_dim = EMBEDDING_DIM,
 input_length= X_train.shape[1],
 weights = [gensim_weight_matrix],trainable = False))
model.add(Conv1D(filters=32, kernel_size=3))
model.add(Dropout(0.6))
model.add(Bidirectional(LSTM(100,return_sequences=True)))
model.add(Dropout(0.6))
model.add(Bidirectional(LSTM(200,return_sequences=True)))
model.add(Dropout(0.6))
model.add(Bidirectional(LSTM(100,return_sequences=False)))
model.add(Dense(class_num, activation = 'softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics = 'accuracy')

model.summary()

stop_early = tf.keras.callbacks.EarlyStopping(
                            monitor='val_loss', patience=10)
checkpoint = tf.keras.callbacks.ModelCheckpoint(
                        'tsea3_model.h5',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_accuracy')
history = model.fit(x=X_train,y= y_train,
              validation_data=(X_test, y_test),
              epochs=1000,
              callbacks=[stop_early, checkpoint])

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(history.history['loss']))

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

print(f'the Accuracy of the model is {max(val_acc)}')

